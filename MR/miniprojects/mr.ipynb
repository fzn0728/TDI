{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "matplotlib.rcParams['savefig.dpi'] = 144"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import grader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mapreduce\n",
    "\n",
    "## Introduciton\n",
    "\n",
    "We are going to be running mapreduce jobs on the wikipedia dataset.  The dataset is available (pre-chunked) on [s3](s3://dataincubator-course/mrdata/simple/).\n",
    "\n",
    "For development, you can even use a single chunk (eg. part-00026.xml.bz2). That is small enough that mrjob can process the chunk in a few seconds. Your development cycle should be:\n",
    "\n",
    "1.  Get your job to work locally on one chunk.  This will greatly speed up your\n",
    "development.  To run on local:\n",
    "```bash\n",
    "python job_file.py -r local data/wikipedia/simple/part-00026.xml.bz2 > /tmp/output.txt\n",
    "```\n",
    "    \n",
    "2.  Get your job to work on the full dataset on GCP (Google Cloud Platform).  This will greatly speed up your production.  To run on GCP ([details](https://pythonhosted.org/mrjob/guides/dataproc-quickstart.html)):\n",
    "```bash\n",
    "python job_file.py -r dataproc data/wikipedia/simple/part-00026.xml.bz2 \\\n",
    "    --output-dir=gs://my-bucket/output/ \\\n",
    "    --no-output \n",
    "```\n",
    "\n",
    "    Not that you can also pass an entire local directory of data (eg. `data/simple/`) as the input.\n",
    "\n",
    "### Note on Memory\n",
    "There's a large difference between developing locally on one chunk and running your job on the entire dataset.  While you can get away with sloppy memory use locally, you really need to keep memory usage down if you hope to be able to complete the miniproject.  Remember, memory needs to be $O(1)$, not $O(n)$ in input.\n",
    "\n",
    "### Multiple Mapreduces\n",
    "You can combine multiple steps by overriding the [steps method](https://pythonhosted.org/mrjob/guides/writing-mrjobs.html#multi-step-jobs).  Usually your mapreduce might look like this\n",
    "```python\n",
    "from mrjob.job import MRJob\n",
    "\n",
    "class SingleMRJob(MRJob):\n",
    "    def mapper(self, key, value):\n",
    "        pass\n",
    "\n",
    "    def reducer(self, key, values):\n",
    "        pass\n",
    "```\n",
    "\n",
    "`MRJob` automatically uses the `mapper` and `reducer` methods.  To specify multiple steps, you need to override the `steps` method:\n",
    "\n",
    "```python\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "class MultipleMRJob(MRJob):\n",
    "    def mapper1(self, key, value):\n",
    "        pass\n",
    "\n",
    "    def reducer1(self, key, values):\n",
    "        pass\n",
    "        \n",
    "    def mapper2(self, key, value):\n",
    "        pass\n",
    "\n",
    "    def reducer2(self, key, values):\n",
    "        pass\n",
    "        \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper1, reducer=self.reducer1),\n",
    "            MRStep(mapper=self.mapper2, reducer=self.reducer2),\n",
    "        ]\n",
    "```\n",
    "\n",
    "As a matter of good style, we recommend that you actually write each individual mapreduce as it's own class.  Then write a wrapper module whose sole job is to combine those mapreduces by overriding `steps`.\n",
    "\n",
    "Some simple boilerplate for this, taking advantage of the default `steps` function that we get for free in a single-step MRJob class:\n",
    "\n",
    "```python\n",
    "class FirstStep(MRJob):\n",
    "  def mapper(self, key, value):\n",
    "    pass\n",
    "  def reducer(self, key, values):\n",
    "    pass\n",
    "  \n",
    "class SecondStep(MRJob):\n",
    "  def mapper(self, key, value):\n",
    "    pass\n",
    "  def reducer(self, key, values):\n",
    "    pass\n",
    "  \n",
    "class SteppedJob(MRJob):\n",
    "  \"\"\"\n",
    "  A two-step job that first runs FirstStep's MR and then SecondStep's MR\n",
    "  \"\"\"\n",
    "  def steps(self):\n",
    "    return FirstStep().steps() + SecondStep().steps()\n",
    "```\n",
    "\n",
    "\n",
    "### Note on Style\n",
    "Here are some helpful articles on how mrjob works and how to pass parameters to your script:\n",
    "  - [How mrjob is run](https://pythonhosted.org/mrjob/guides/concepts.html#how-your-program-is-run)\n",
    "  - [Adding passthrough options](https://pythonhosted.org/mrjob/job.html#mrjob.job.MRJob.add_passthrough_option)\n",
    "  - [An example of someone solving similar problems](http://arunxjacob.blogspot.com/2013/11/hadoop-streaming-with-mrjob.html)\n",
    "\n",
    "See the notebook \"Hadoop MapReduce with mrjob\" in the datacourse for more details.\n",
    "\n",
    "Finally, if you are find yourself processing a lot of special cases, you are probably doing it wrong.  For example, mapreduce jobs for `Top100WordsSimpleWikipediaPlain`, `Top100WordsSimpleWikipediaText`, and `Top100WordsSimpleWikipediaNoMetaData` are less than 150 lines of code (including generous blank lines and biolerplate code)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1: top100_words_simple_plain\n",
    "Return a list of the top 100 words in an article text (in no particular order). You will need to write this as two map reduces:\n",
    "\n",
    "1. The first job is similar to standard wordcount but with a few tweaks. The data provided for wikipedia is in `*.xml.bz2` format.  Mrjob will automatically decompress `bz2`.  We'll deal with the `xml` in the next question. For now, just treat it as text.  A few hints:\n",
    "   - To split the words, use the regular expression \"\\w+\".\n",
    "   - Words are not case sensitive: i.e. \"The\" and \"the\" reference to the same word.  You can use `string.lower()` to get a single case-insenstive canonical version of the data.\n",
    "\n",
    "2. The second job will take a collection of pairs `(word, count)` and filter for only the highest 100.  A few notes:\n",
    "    - **Passing parameters:** To make the job more reusable make the job find the largest `n` words where `n` is a parameter obtained via [`get_jobconf_value`](https://pythonhosted.org/mrjob/utils-compat.html).\n",
    "    - **Keeping track of the top n:** We have to keep track of at most the `n` most popular words.  As long as `n` is small, e.g. 100, we can keep track of the *running largest n* in memory wtih a priority-queue. We suggest taking a look at `heapq` ([details](https://docs.python.org/2/library/heapq.html)), part of the Python standard library for this.  It allows you to push elemnets into a list while keeping track of the highest priority element.\n",
    "```python\n",
    "h = []\n",
    "heappush(h, (5, 'write code'))\n",
    "heappush(h, (7, 'release product'))\n",
    "heappush(h, (1, 'write spec'))\n",
    "heappush(h, (3, 'create tests'))\n",
    "heappop(h)  // returns (1, 'write spec')\n",
    "```\n",
    "   \n",
    "       A naive implementation would cost $O(1)$ to insert but $O(n)$ to retrieve.  `heapq` uses a [self-balancing binary search tree](https://en.wikipedia.org/wiki/Self-balancing_binary_search_tree) to enable $O(\\log(n))$ insertion and $O(1)$ removal. You may be asked about this data structure on an interview so it is good to get practice with it now.\n",
    "    - **Working across nodes:** To obtain the largest `n`, we need to first obtain the largest n elements per chunk from the mapper, output them to the same key (reducer), and then collect the largest n elements of those in the reducer (**Question:** why does this gaurantee that we have found the largest n over the entire set?)\n",
    "    - **Working within a node:** Given that we are using a priority queue, we will need to first initialize it, then `push` or `pushpop` each record to it, and finally output the top `n` after seeing each record.  For mappers, notice that these three phases correspond nicely to these three functions:\n",
    "        - `mapper_init`\n",
    "        - `mapper`\n",
    "        - `mapper_final`\n",
    "\n",
    "    There are similar functions in the reducer.  Also, while the run method to launch the mapreduce job is a classmethod:\n",
    "        ```python\n",
    "          if __name__ == '__main__':\n",
    "            MRWordCount.run()\n",
    "        ```\n",
    "     actual instances of our mapreduce are instantiated on the map and reduce nodes.  More precisely, a separate mapper class is instantiated in each map node and a reducer class is instantiated in each reducer node.  This means that the three mapper functions can pass state through `self`, e.g. `self.heap`. Remember that to pass state between the map and reduce phase, you will have to use `yield` in the mapper and read each line in the reducer. (**Question:** Can you pass state between two mappers?)\n",
    "\n",
    "**Checkpoint:**\n",
    "- Total unique words: 1,584,646"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "l = []\n",
    "with open('/home/vagrant/datacourse/MR/output.txt') as f:\n",
    "    lines = f.read().replace('\"', '').splitlines()\n",
    "output_t = [tuple(x.split('\\t')) for x in lines]\n",
    "for i in range(200):\n",
    "    l.append(tuple((output_t[i][0],int(output_t[i][1]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 1572530),\n",
       " ('quot', 1400092),\n",
       " ('gt', 1211881),\n",
       " ('lt', 1205636),\n",
       " ('id', 1142726),\n",
       " ('of', 972021),\n",
       " ('in', 658172),\n",
       " ('text', 604105),\n",
       " ('title', 539585),\n",
       " ('to', 488669),\n",
       " ('page', 439533),\n",
       " ('is', 406945),\n",
       " ('format', 386248),\n",
       " ('model', 381440),\n",
       " ('revision', 380458),\n",
       " ('category', 380445),\n",
       " ('ns', 378528),\n",
       " ('timestamp', 377859),\n",
       " ('contributor', 377213),\n",
       " ('sha1', 376839),\n",
       " ('ref', 370300),\n",
       " ('username', 361648),\n",
       " ('comment', 349660),\n",
       " ('parentid', 292767),\n",
       " ('on', 243819),\n",
       " ('by', 218299),\n",
       " ('http', 217151),\n",
       " ('for', 215453),\n",
       " ('was', 213271),\n",
       " ('it', 209568),\n",
       " ('x', 201333),\n",
       " ('wiki', 199371),\n",
       " ('space', 197034),\n",
       " ('xml', 189057),\n",
       " ('preserve', 188765),\n",
       " ('wikitext', 188559),\n",
       " ('name', 179580),\n",
       " ('d', 157977),\n",
       " ('that', 157381),\n",
       " ('www', 156201),\n",
       " ('s', 154554),\n",
       " ('font', 153498),\n",
       " ('user', 153378),\n",
       " ('from', 152592),\n",
       " ('with', 137737),\n",
       " ('style', 136494),\n",
       " ('he', 125011),\n",
       " ('this', 122896),\n",
       " ('com', 122511),\n",
       " ('color', 121251),\n",
       " ('talk', 120486),\n",
       " ('minor', 117012),\n",
       " ('or', 112445),\n",
       " ('center', 111244),\n",
       " ('redirect', 111060),\n",
       " ('i', 103372),\n",
       " ('small', 102739),\n",
       " ('not', 93981),\n",
       " ('new', 93612),\n",
       " ('people', 91245),\n",
       " ('football', 90061),\n",
       " ('nbsp', 89131),\n",
       " ('united', 86196),\n",
       " ('cite', 82964),\n",
       " ('first', 82827),\n",
       " ('other', 82109),\n",
       " ('date', 82018),\n",
       " ('url', 79552),\n",
       " ('if', 76728),\n",
       " ('they', 75749),\n",
       " ('his', 75599),\n",
       " ('span', 74817),\n",
       " ('now', 74414),\n",
       " ('states', 72458),\n",
       " ('stub', 70339),\n",
       " ('one', 70215),\n",
       " ('moved', 69238),\n",
       " ('have', 69018),\n",
       " ('jpg', 68160),\n",
       " ('utc', 67562),\n",
       " ('c', 66964),\n",
       " ('links', 66264),\n",
       " ('has', 64517),\n",
       " ('right', 64126),\n",
       " ('web', 62463),\n",
       " ('image', 60500),\n",
       " ('publisher', 60136),\n",
       " ('player', 59890),\n",
       " ('template', 58847),\n",
       " ('interwiki', 58262),\n",
       " ('provided', 58070),\n",
       " ('wikidata', 57277),\n",
       " ('can', 56453),\n",
       " ('which', 56187),\n",
       " ('n', 55927),\n",
       " ('de', 55773),\n",
       " ('sup', 55726),\n",
       " ('file', 55407),\n",
       " ('but', 55314),\n",
       " ('county', 55054)]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l[:100]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================\n",
      "Your score:  0.76\n",
      "==================\n"
     ]
    }
   ],
   "source": [
    "def top100_words_simple_plain():\n",
    "    # return [(\"the\", 1586419)] * 100\n",
    "    return l[:100]\n",
    "grader.score(question_name='mr__top100_words_simple_plain', func=top100_words_simple_plain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: top100_words_simple_text\n",
    "Notice that the words \"page\" and \"text\" make it into the top 100 words in the previous problem.  These are not common English words!  If you look at the xml formatting, you'll realize that these are xml tags.  You should parse the files so that tags like `<page></page>` should not be included in your total, nor should words outside of the tag `<text></text>`.\n",
    "\n",
    "**Hints**:\n",
    "1. Both `xml.etree.elementtree` from the Python stdlib or `lxml.etree` parse xml. `lxml` is significantly faster though and avoids some bugs.\n",
    "\n",
    "2. In order to parse the text, we will have to accumulate a `<page></page>` worth of data and then split the resulting string into words.\n",
    "\n",
    "3. Don't forget that the Wikipedia format can have multiple revisions but you only want the latest one.\n",
    "\n",
    "4. What happens if a content from a page is split across two different mappers? How does this problem scale with data size?\n",
    "\n",
    "**Checkpoint:**\n",
    "- Total unique words: 867,871"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def top100_words_simple_text():\n",
    "    return [(\"the\", 1577579)] * 100\n",
    "\n",
    "grader.score(question_name='mr__top100_words_simple_text', func=top100_words_simple_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3: top100_words_simple_no_metadata\n",
    "\n",
    "Finally, notice that 'www' and 'http' make it into the list of top 100 words in the previous problem.  These are also not common English words either!  These are clearly from the url in hyperlinks.  Looking at the format of [Wikipedia links](http://en.wikipedia.org/wiki/Help:Wiki_markup#Links_and_URLs) and [citations](http://en.wikipedia.org/wiki/Help:Wiki_markup#References_and_citing_sources), you'll notice that they tend to appear within single and double brackets and curly braces.\n",
    "\n",
    "**Hint**:\n",
    "You can either write a simple parser to eliminate the urls within brackets, angle braces, and curly braces or you can use a package like the colorfully-named [mwparserfromhell](https://github.com/earwig/mwparserfromhell/), which has been provisioned on `mrjob` and supports the convenient helper function `strip_code()` (which is used by the reference solution).\n",
    "\n",
    "**Checkpoint:**\n",
    "- Total unique words: 618,410"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def top100_words_simple_no_metadata():\n",
    "    return [(\"the\", 1427342)] * 100\n",
    "\n",
    "grader.score(question_name='mr__top100_words_simple_no_metadata', func=top100_words_simple_no_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4: wikipedia_entropy\n",
    "The [Shannon entropy](https://en.wikipedia.org/wiki/Entropy_(information_theory) of a discrete random variable with probability mass function $p(x)$ is:\n",
    "\n",
    "$$H(X) = - \\sum p(x) \\log_2 p(x)$$\n",
    "\n",
    "You can think of the Shannon entropy as the number of bits needed to represent the random variable if it were optimally compressed.  It is also closely tied to the notion of entropy from physics.\n",
    "\n",
    "You'll be estimating the Shannon entropy of different Simple English and Thai based off of their Wikipedias. Do this with n-grams of characters, by first calculating the entropy of a single n-gram and then dividing by n to get the per-character entropy. Use n-grams of size 1, 2, 3.  How should our per-character entropy estimates vary with n?  How should they vary by\n",
    "the size of the corpus? How much data would we need to get reasonable entropy estimates for each n?\n",
    "\n",
    "The data you need is available at:\n",
    "    - https://s3.amazonaws.com/dataincubator-course/mrdata/simple/part-000\\*\n",
    "    - https://s3.amazonaws.com/dataincubator-course/mrdata/thai/part-000\\*\n",
    "\n",
    "*Question*: Why do we need to use map-reduce? There are >300 million characters in this dataset. How much memory would it take to store all `n`-grams as `n` increases?\n",
    "\n",
    "Notes:\n",
    "1. Characters are case sensitive.\n",
    "1. Do not use the previous regex `\\w+` to split --- depending on your system configuration, this may only match English characters, which would severely skew entropy estimates for Thai. Be careful about unicode.\n",
    "1. Please treat all whitespace as the same character.  You can do this by\n",
    "  `\" \".join(text.split())`\n",
    "1. For reference, the exact code we use to extract text is:\n",
    "\n",
    "```\n",
    "    wikicode = mwparserfromhell.parse(text)\n",
    "    text = \" \".join(\" \".join(fragment.value.split())\n",
    "                    for fragment in wikicode.filter_text())\n",
    "```\n",
    "\n",
    "A naive implementation of this job will take a very long time to run.  Instead, we will need to use a few optimizations:\n",
    "1. See [this post](http://www.johndcook.com/blog/2013/08/17/calculating-entropy/) on how to calculate entropy efficiently.\n",
    "2. It turns out that writing to disk is the most expensive part of a map-reduce.  [Zipf's law](https://en.wikipedia.org/wiki/Zipf's_law) tells us that only a handful (relatively-speaking) of n-grams make up most of our observations.  Can you do a map-side cache of these values to reduce the number of writes?\n",
    "   \n",
    "   Note that it may not be possible to match the reference solution by omitting n-grams.\n",
    "3. Entropy is a function of the count distribution, i.e. it is independent of which ngrams correspond to which counts.  If we have N singleton ngrams, it's more efficient to (somehow) encode that as \"N singleton ngrams\" rather than as N key-value pairs:\n",
    "```\n",
    "    (word1, 1)\n",
    "    (word2, 1)\n",
    "    (word3, 1)\n",
    "    ...\n",
    "```\n",
    "   Can you use a in-memory cache to solve this problem?  What fraction of ngrams only occur once?  How much of a speedup do you expect to get from this optimization?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def wikipedia_entropy():\n",
    "    return [\n",
    "        (\"simple1\", 1.),\n",
    "        (\"simple2\", 1.),\n",
    "        (\"simple3\", 1.),\n",
    "        (\"thai1\", 1.),\n",
    "        (\"thai2\", 1.),\n",
    "        (\"thai3\", 1.),\n",
    "    ]\n",
    "\n",
    "grader.score(question_name='mr__wikipedia_entropy', func=wikipedia_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5: link_stats_simple\n",
    "Let's look at some summary statistics on the number of unique links on a page to other Wikipedia articles.  Return the number of articles (count), average number of links, standard deviation, and the 25%, median, and 75% quantiles.\n",
    "\n",
    "1. Notice that the library `mwparserfromhell` supports the method `filter_wikilinks()`.\n",
    "2. You will need to compute these statistics in a way that requires O(1) memory.  You should be able to compute the first few (i.e. non-quantile) statistics exactly by looking at the first few moments of a distribution. The quantile quantities can be accurately estimated by using reservoir sampling with a large reservoir.\n",
    "3. If there are multiple links to the article have it only count for 1.  This keeps our results from becoming too skewed.\n",
    "4. Don't forget that some (a surprisingly large number of) links have unicode! Make sure you treat them correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def link_stats_simple():\n",
    "    return [\n",
    "        (\"count\", 0.),\n",
    "        (\"mean\", 0.),\n",
    "        (\"stdev\", 0.),\n",
    "        (\"25%\", 0.),\n",
    "        (\"median\", 0.),\n",
    "        (\"75%\", 0.),\n",
    "    ]\n",
    "\n",
    "grader.score(question_name='mr__link_stats_simple', func=link_stats_simple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6: link_stats_english\n",
    "The same thing but for all of English Wikipedia.  This is the real test of how well your algorithm scales!  The data is also located on [s3](s3://dataincubator-course/mrdata/english/).\n",
    "\n",
    "**Note:**\n",
    "Because of the size of the dataset, this job may take several hours to complete. It's advisable to run it overnight once you're reasonably sure it will work (due to testing the code on smaller inputs).\n",
    "\n",
    "As a barometer, our reference solution takes around 5 hours to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def link_stats_english():\n",
    "    return [\n",
    "        (\"count\", 0.),\n",
    "        (\"mean\", 0.),\n",
    "        (\"stdev\", 0.),\n",
    "        (\"25%\", 0.),\n",
    "        (\"median\", 0.),\n",
    "        (\"75%\", 0.),\n",
    "    ]\n",
    "\n",
    "grader.score(question_name='mr__link_stats_english', func=link_stats_english)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 7: double_link_stats_simple\n",
    "Instead of analyzing single links, let's look at double links.  That is, pages `A` and `C` that are connected through many pages `B` where there is a link `A -> B -> C` or `C -> B -> A`. Find the list of all pairs `(A, C)` (you can use alphabetical ordering to break symmetry) that have the 100 \"most\" connections (see below for the definition of \"most\").  This should give us a notion that the articles `A` and `C` refer to tightly related concepts.\n",
    "\n",
    "1. This can be thought of as a Matrix Multiplication problem.  If the adjacency matrix is denoted $M$ (where $M_{ij}$ represents the link between $i$ an $j$), we are looking for the highest 100 elements of the matrix $M M$. Note that this doesn't mean constructing matrices is the most efficent way to solve the problem.\n",
    "\n",
    "2. Notice that a lot of Category pages (denoted \"Category:.*\") have a high link count and will rank very highly according to this metric.  Wikipedia also has `Talk:` pages, `Help:` pages, and static resource `Files:`.  All such \"non-content\" pages (and there might be more than just this) and links to them should be first filtered out in this analysis.\n",
    "\n",
    "3. Some pages have more links than others.  If we just counted the number of double links between pages, we will end up seeing a list of articles with many links, rather than concepts that are tightly connected.\n",
    "\n",
    "   1. One strategy is to weight each link as $\\frac{1}{n}$ where $n$ is the number links on the page.  This way, an article has to spread it's \"influence\" over all $n$ of its links.  However, this can throw off the results if $n$ is small.\n",
    "\n",
    "   2. Instead, try using [Bayesian Smoothing](https://en.wikipedia.org/wiki/Additive_smoothing) by weighting each link as $\\frac{1}{n+10}$ where 10 sets the \"scale\" in terms of number of links above which a page becomes \"significant\".  The number 10 was somewhat arbitrarily chosen but seems to give reasonably relevant results.\n",
    "\n",
    "   3. This means that our \"count\" for a pair A,C will be the products of the two link weights between them, summed up over all their shared connections.\n",
    "\n",
    "4. Again, if there are multiple links from a page to another, have it only count for 1.  This keeps our results from becoming skewed by a single page that references the same page multiple times.\n",
    "\n",
    "5. You'll need to filter out the cases where A -> B -> A, but just note that the reference solution does this after calculating the link weights.\n",
    "\n",
    "6. The links and page titles are encoded into utf-8.\n",
    "\n",
    "Don't be afraid if these answers are not particularly insightful.  Simple Wikipedia is not as rich as English Wikipedia.  However, you should notice that the articles are closely related conceptually.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def double_link_stats():\n",
    "    return [((\"idaho\", \"list of cities in idaho\"), 0.055922694796753325)] * 100\n",
    "\n",
    "grader.score(question_name='mr__double_link', func=double_link_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Copyright &copy; 2016 The Data Incubator.  All rights reserved.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
